\chapter{Rephotography Application}

This chapter will document the development of the iOS application implementing
the theory outlined in the previous chapter. The iOS platform will be briefly
introduced before the user interface, features and implementation of the app
will be described.

\section{iOS Overview}
% I took this from http://tex.stackexchange.com/a/45302/31250
% Strangely, applying http://tex.stackexchange.com/a/44362/31250 in the preamble
% did not work
\newcommand*{\code}[1]{\begingroup\ttfamily\hyphenchar\font=23{#1}\endgroup}

iOS is the operating system running on all of Apple's mobile devices. It is
currently\footnote{\today\ \citep{ios8}} at version 8.4.1 with its successor iOS
9 in beta stadium. Code is compiled by the clang compiler families and thus all
languages supported by this compiler can be used. The primary development
language is Objective-C, a strict, object-oriented superset of the C language\footnote{There is
   no precise definition which C version it is a superset of, except that it is
   ANSI standardised; For each C version supported by the compiler, Objective-C
will be compatible with it.}. Code of these languages can be freely mixed. An
Objective-C++ dialect exists to support C++ as well.

The software development kit for the platform employs Objective-C for most
high-level APIs and C for more low-level functionality. In regards to
application design, the Model-View-Controller (see \autoref{fig:mvc}) pattern is
used throughout the Cocoa library which incorporates the standard
(\code{Foundation} classes) and graphical user interface (\code{AppKit} on OSX,
\code{UIKit} on iOS) libraries as well as \code{Core Data} for persistence. For
applications with little or no need for data manipulation, the controller
objects can also fill the role of the model.  For every view hierarchy, a view
controller must exist to present it to the user and mediate interaction with
them.

\begin{figure}[h]
   {\centering      
      \input{gfx/mvc.tex}
      \caption{Model-View-Controller pattern}
   \label{fig:mvc}}
\end{figure}

Generally, an iOS application is a sequence of view controllers presented to the
user in various ways---some may be full-fledged screens, while others are only
presented modally. Apple's XCode integrated development environment allows to
visually model the flow of the application by use of \code{Storyboard}s. These
are XML files which define controllers and their relationships. While
it is equally possible to specifiy the presentation order programmatically, this
allows for a cleaner separation between user interface and business logic.
Storyboards consist of a number of view controllers with associated views which
are connected by \code{Segue}s. A Segue is triggered by actions like a button
press and will present the next view controller to the user. The Storyboard for
the application developed in this work is shown in \autoref{fig:storyboard}.

\begin{figure}[h]
   {\centering      
      \input{gfx/storyboard.tex}
      \caption[Storyboard example]{The Storyboard of this application. There is
      always a root view controller on whose stack new controllers are pushed or popped from.}
   \label{fig:storyboard}}
\end{figure}

\section{Software Modules}

Although Objective-C does not support modularisation by packages, namespace or
similar, the software can be divided into five parts.

\subsection{User Interface}

For the user interface, several custom views classes are implemented.
\begin{enumerate}
   \item \code{ArrowView} A view which is backed by an \code{ArrowLayer} and
      allows to display it as \code{CALayer}s cannot be shown without a
      wrapping \code{UIView}.
   \item \code{ArrowLayer} A \code{CALayer} subclass which can display a
      parametrisable arrow shape whose parameters can be seamlessly animated.
      \code{CALayers} are the backbone of all \code{UIView} objects.
   \item \code{ImageScrollView} A \code{ScrollView} subclass which allows to
      display a zoomable and pannable image while also preserving the visible
      portion when the user interface orientation changes.
   \item \code{CircleView} A simple view purely for drawing a circle parametrised
      with colour and line width.
   \item \code{MaskableUIImageView} An \code{UIImageView} which allows the
      image to be clipped to some percentage of its width. This is used to
      display a before-after comparison of the original and repeat photographs.
\end{enumerate}

Furthermore, two view hierarchies and the Storyboard belong to this module.  If
a custom view must exhibit some particular behviour which necessitates custom
methods, creating classes is appropriate. For interface elements which contain a
larger hierarchy of nested views without exposing any particular behaviour,
specifying them in a visual fashion is less involved. Such view hierarchies can
be built visually as \code{XIB} files in an XML format. The
\code{CameraOverlay} overlayed on the current camera picture is created like
this, as well as the launch screen shown when starting the app.

\subsection{Image Processing}

For all image processing, the OpenCV library used in version 3.0. The library
can be natively used from C++, but Python and Java bindings as well as a
deprecated C API exist. OpenCV encodes images in its \code{Mat} type which is
the only type perforating other parts of the application. 
While it is possible to mix Objective-C and C++ code,
using C++ types in header files forces every client including them to also be
compiled as Objective-C++ which may be unwanted. For instance, XCode's
refactoring tools cannot be used on these sources. It is thus reasonable to create
wrapper classes for all C++ types encapsulating the access to the data so that
the interface is pure Objective-C. Only the implementation of the wrapper class
would need to be compiled as C++, no client would be affected. Ideally, all
access to C++ types could be mediated this way, but often this would necessitate 
a large amount of boilerplate code to translate Objective-C messages in C++
calls to the wrapped object.
It is therefore convenient to encapsulate a C++ type but make it still accessible to
client classes if needed. 

One solution is a variant of the \emph{Pointer-to-Implementation} pattern
(PIMPL, also known as \emph{Bridge} in \citep{gamma1995}). The wrapper class'
interface (\autoref{lst:pimpl}) is specified in pure Objective-C and contains a
opaque pointer to a C \code{struct} representing the wrapped data as a C type
which therefore does not lead to compatibility issues as all C code is valid
Objective-C. For the original pattern, this allows modifying the wrapper
hierarchy independently of the implementation hierarchy (e.g. different
implementations could be behind the pointers, for different platforms). The
definition of this pointer-to-implementation type is placed in a second header
file (\autoref{lst:pimpl2}) and is imported only by those clients which actually
need to access the data, not only pass it on. This permits to compile only those
source files as Objective-C++ which must deal with the actual OpenCV \code{Mat}.
In principle, it would be possible to create methods in the wrapper class for
all uses of the C++ type throughout the program. However, this would waste the
benefits which e.g. C++ operator overloading yields for readability. Since all
image processing is done in C++ anyway, it is more economic to simply provide
access to the wrapped data to those classes that need it.

\lstset{
   language=[Objective]C,
   float,
}

\begin{lstlisting}[
   caption={\code{CVMatWrapper.h}. The wrapper interface contains an opaque pointer to a C
      \code{struct}. \code{\#include}ing this header does not invalidate
   othewise valid Objective-C code.},
   label={lst:pimpl},
]
struct CVMatWrapperImpl;
@interface CVMatWrapper : NSObject
@property (nonatomic,readwrite) struct CVMatWrapperImpl* impl; ///< Pointer to struct wrapping the matrix
@property (nonatomic,readonly) int rows; ///< Number of rows of the wrapped matrix
@property (nonatomic,readonly) int cols; ///< Number of columns of the wrapped matrix
-(NSArray*)eulerAngles;
-(NSString*)description;
@end
\end{lstlisting}

\begin{lstlisting}[
   caption={\code{CVMatWrapperImpl.h}. The implementation header defines the actually wrapped type.
      Including it will force a client to compile as Objective-C++ since a C++
      type is used. Every class which deals with the data itself can include this
   header in addition to the wrapper one.},
   label={lst:pimpl2},
]
struct CVMatWrapperImpl
{
   cv::Mat cvMatrix; ///< The wrapped matrix
};
\end{lstlisting}

\begin{lstlisting}[
   caption={\code{CVMatWrapper.mm}. The wrapper class' implementation must be Objective-C++ and can
   encapsulate all access to the underlying data, if necessary.},
   label={lst:pimpl3},
]
#import "CVMatWrapper.h"
#import "CVMatWrapperImpl.h"

@implementation CVMatWrapper
-(instancetype)init
{
   self = [super init];
   if (self) self.impl = new CVMatWrapperImpl; // allocate the struct wrapping the matrix
   return self;
}

-(void)dealloc
{
   if (self.impl)
   delete self.impl;
}

-(NSString*)description
{
   //...
}

-(NSArray*)eulerAngles
{
   // ...
}

-(int)rows
{
   return (self.impl->cvMatrix).rows;
}

-(int)cols
{
   return (self.impl->cvMatrix).cols;
}
@end
\end{lstlisting}

The bulk of the work is done inside the utility class' \code{ImageUtils} static
methods. To compute the pose difference between two frames, clients can call the
public method
\code{+[computePoseDifferenceBetweenCamera:secondCamera:calibrationFile:scale:]}.
The two images must be passed, as well as the path to a calibration file in
OpenCV's \code{FileStorage} format. A scale parameter can be used to speed up
the computation by downsampling the images.  The scale is set to $0.33$
compromise between speed and matching robustness, taking into account that
compared to the computations performed on a computer (see
\autoref{ch:evaluation}) some detail is lost due to conversion between iOS's
\code{UIImage} format and OpenCV's \code{Mat} type.

Internally, the method converts the images, uses OpenCV image processing
functions to compute the pose difference and the average distance of the matched
features to the first camera. The client is returned an \code{NSDictionary}
containing the rotation, translation, average point distance, and---should the
computation fail---an error code in which case all other values are invalid.

The method first runs \texttt{AKAZE} on both image, the parameters used are
shown in \autoref{tab:akaze_params}.

\rowcolors{2}{gray!5}{white} % alternating colours
\begin{table}
   \begin{center}
      \begin{tabular}{>{\ttfamily}ll}
         \rowcolor{white}
         \toprule
         \rmfamily Parameter     & Value \\
         \midrule
         nOctaves                & $4$ \\
         threshold               & 0.001 \\
         nOctaveLayers/sublevels & $4$ \\
         descriptor\_size        & $486$\tablefootnote{\texttt{AKAZEFeatures.cpp line $720$}} bits \\
         descriptor\_channels    & $3$ \\
      \end{tabular}
      \caption{Parameters used for \texttt{AKAZE}}
      \label{tab:akaze_params}
   \end{center}
\end{table}

\subsection{View Controllers}

\subsection*{\code{MainViewController}}

\newcommand*{\button}[1]{
   \tikzexternaldisable
   \tikz[baseline=-.5ex]{\node[inner sep=1pt,outer sep=0,draw,rounded corners=2pt]{\small\texttt{#1}};}
   \tikzexternalenable
}

When the app starts up, the user is shown a \code{MainViewController} which
offers a choice whether to make a new rephotograph or view existing ones.
Clicking the \button{Take Rephoto} button will trigger a segue to a
\code{PhotoChooserController}, while tapping \button{View Rephotos} will modally
present an \code{ELCImagePickerController}.

\subsection*{\code{PhotoChooserController}}

This view controller displays a scrollable and pannable view and a button
\button{Choose Photo from Library} to load a reference photograph into it. The
photo is picked from the user's photo album.  Furthermore, the navigation bar
contains a button to access the settings
(\raisebox{-3pt}{\includegraphics[width=1em]{gfx/settings_icon.png}}, presents a
\code{SettingsViewController}) as well as a button to start the rephotograph
(\raisebox{-1pt}{\includegraphics[width=1em]{gfx/cam_icon.png}}). The latter
will modally present a \code{UIImagePicker- Controller} and attach to it a
\code{RephotoMangager} that controls the rephotography.

When an image is picked, the controller uses the Canny edge detection algorithm
\citep{canny1986} to generate an overlay which will then help the user in
finding the perfect aligment.

\subsection*{\code{SettingsViewController}}

The \code{SettingsViewController} possesses three panels which allow to
modify the color of the edge overlay and insert authentication data for
future use with an envisioned online platform. The third panel is unused in the current
version and can be used for other kinds of preferences such as the default type
of visualisation for the necessary camera motion.

\subsection*{\code{ResultViewController}}

This controller must be passed a completed rephotograph in an \code{Image- Data}
object and displays the new image over the orginal. With a slider, the new image
is horizontally clipped to some percentage of it width and can thus be
dynamically unrolled over the original.

On the top right, a share button (\raisebox{-3pt}{\includegraphics[width=.8em]{gfx/share.png}})
allows to upload a rephoto to an experimental test server for display, but the
functionality is only rudimentary.

\subsection*{\code{ELCImagePickerController}}

For the task of reviewing rephotographs, the \code{ELCImagePickerController} is
used. Contrarily to the built-in \code{UIImagePickerController}, this
reimplementation by \citet{nutting2013} allows to display a specific asset, like
a photo album instead of all assets there are. In the album display, the user is
shown the new photographs of each rephoto and upon selection is presented a
\code{ResultViewController} with the rephotograph.

\subsection{Categories}

Code reuse in Objective-C can be accomplished by inheritance as in other
object-oriented languages, but often, when only one piece of behaviour is to be
added, not overridden\footnote{The behaviour is undefined when a Category method
has the same signature as one of the class \citep{customizing}}, the
lightweight Category concept is employed. A Category on a class adds methods
or properties to that class The header file declaring the category can be
included by all clients that wish to make use of the modified or added
behaviour, and left out by all others. This way, no unnecessarily large class
hierarchy is created.

In this application, categories are used for different purposes. They are
summarised in \autoref{tab:categories}.

\begin{table}
   \begin{tabularx}{\linewidth}{>{\ttfamily}p{4cm}X}
      \toprule
      \rowcolor{white}
      {\rmfamily Category Name}           & Function \\
      \midrule
      NSMutableURLRequest (RephotoUpload) & Adds method to post a rephoto to an
      experimental server via HTTP \\
      ALAssetsLibrary (CustomPhotoAlbum)  & Adds method to add an image to a
      specific album, not possible per default. Used to save rephotos to the
      \code{Rephotos} album \\
      NSUserDefaults (UIColor)            & By default, only some types can be
      persistently saved in the user defaults. Adds method to save colours. Used
      to remember the user's edge colour.\\
      CALayer (UIColor)                   & Properties for view objects can be
      set in Interface Builder, but only if they are of object type. To set a
      view's backing layer's border colour outside of the code, a colour
      property of object type is defined, mapped to the C type \code{CGColor}
      used by \code{CALayer}. \\
      UIBezierPath (Arrow)                & Adds a method to generate an arrow
      shape with a bézier curve. Used for displaying user guidance. \\
      UIImage (Zoom)                      & Adds a method to crop an image to a
      rectangle centered on the image center, effectively zooming in. Used to
      allow the user to zoom the camera image. \\
      UIScrollView (Center)               & Adds methods to center the content
      and zoom to a particular size. Used fo display of the original image. \\
      UIColor (CustomColors)              & Contrarily to Android, iOS possesses
      no Resources framework, so this Category declares some commonly used
      colours for the user interface \\
      UIView (Effects)                    & Adds some animation methods to all
      views to avoid repeating boilerplate code. \\
      \bottomrule
   \end{tabularx}
   \caption{Summary of categories}
   \label{tab:categories}
\end{table}

\FloatBarrier

\subsection{Other Classes -- \code{RephotoManager}}

The two most important classes are the \code{RephotoManager} and the
\code{Pose- DifferenceEstimationOperation}. A \code{RephotoManager} is attached to
a \code{UIImagePickerController} in camera mode and controls the execution of
the rephotograph. The camera overlay contains a shutter button and information
labels telling the user what to do next. Tapping the info button
(\raisebox{-2pt}{\includegraphics[width=1em]{gfx/info.png}}) expands the label
to reveal more information. The manager is realised as a state machine, depicted
as a flow chart in \autoref{fig:rephoto_manager}.

\begin{figure}[h]
   {\centering      
      \input{gfx/rephoto_manager.tex}
   \caption{States of the rephoto manager object}
   \label{fig:rephoto_manager}}
\end{figure}

\subsubsection*{\code{START}}

Initially, the user interface presents shutter and cancel buttons in a toolbar
at the bottom and an info indictator on the top left, telling the user the take
the first frame. When the user taps the shutter button, the first frame is
captured and saved.

\subsubsection*{\code{HAS\_FIRST\_FRAME}}

When transitioning to this state, the manager changes the info label accordingly
and displays a check mark for the first frame in the centre of the toolbar.
Furthermore, at this point the precomputed edge overlay is presented so that
taking the second frame which shoul be reasonably close to the original
viewpoint is possible.

\subsubsection*{\code{HAS\_SECOND\_FRAME}}

Once the second frame is captured, the second checkmark is shown, the info
label changes again and the preprocessing step is attempted. A
\code{PoseDifferenceEstimationOperation} is started with the first and reference
frames and another for the reference scale computation by use of first and
second frames. Relative translation, rotation and goal scale are saved.

Should one of the computations fail, the UI resets to the initial state execpt
for an error label at the bottom centre informing the user that the matching
failed. Otherwise a timer is initiated that will take pictures at a certain
interval which should accord with the hardwary running the program. For the
tested device, it is set to $1$ second, although somewhat higher frame rates are
possible.

Meanwhile, a panel at the bottom centre is revealed which visualises the
necessary motion of the camera. For this, several ideas are possible.
\citet{bae2010} found that an arrow visualisation for the translation in the
XY-plane and Z-direction is most effective so this approach has been chosen
here. 

\subsubsection*{\code{DONE}}

After the final image is captured, the manager informs its delegate (in this
case the \code{PhotoChooserController}) that its task is finished and hands over
the final capture.

\subsection{Other Classes -- \code{PoseDifferenceEstimationOperation}}

iOS supports different APIs for parallelisation. Besides manual thread creation,
Grand Central Dispatch allows to easily dispatch blocks (function objects) to
different kinds of queues. The runtime automatically removes blocks from the
queue for execution. Dispatch queues can be serial or concurrent and both
synchronous and asynchronous dispatch is possible as well as synchronisation,
but the API is not object-oriented and some useful features are unavailable.  Is
not possible to specify how many blocks should be executed in parallel or to
cancel tasks after submission. Since the application should make optimal use of
the hardware due to high computational demand while not queueing up many tasks,
or tasks which are obsolete, such functionality is desirable.

Operation queues sacrifice some efficiency, but expose a more high-level
object-oriented interface in which \code{NSOperation} objects are submitted to
\code{NSOperationQueue}s and their \code{maxConcurrentOperationCount} property
allows to set a maximum number of concurrent operations. The application sets
this limit to $3$ for an iPad (assuming $3$ cores like an iPad Air 2) and $2$
for iPhones, as the current devices possess two cores. Since there is no
concurrent modification of data and tasks are started in slow intervals,
explicit sychronisation is not needed.

A \code{PoseDifferenceEstimationOperation} is created with two images (one of
which will always be the first frame), a scale parameter for downsampling and
computed values for the relative pose between first and reference frames and the
goal scale. Furthermore, a block is passed which can accept three float
parameters---one for an angle around the optical axis, one around the vertical
axis, and one for the scale of the necessary translation. The intended purpose
of this block is to drive some kind of visualisation. The operation then computes the
relative pose and the necessary translation as in \eqref{eq:necessary_trans}. 

If $\sub{T}{current,ref} = (x,y,z)$\footnote{One should note that this
   calculation depends on the coordinate system. In OpenCV, the origin is at the
   top left corner of an image, the positive $x$-axis is the image width, the
   positive $y$-axis the image height and points downward. In a more canonical
   representation, the $y$-value thus may be flipped}, then the vector direction
in th XY-plane is given by 
\begin{equation*}
   \alpha = \atan2{(y,x)}
\end{equation*}
